{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本分类模型的构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载训练集并查看样本数量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "      <td>sen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>【爱情语句】</td>\n",
       "      <td>恬恬静静温柔，端端正正品优，实实在在德厚。清清秀秀，纯纯洁洁明眸。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>【爱情语句】</td>\n",
       "      <td>你喜欢长长的夏季吗?因为一阵风，因为一场梦。哈哈 ，我喜欢你。清晨，午后和伴晚。都喜欢你!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【爱情语句】</td>\n",
       "      <td>我喜欢你，所以静静的等你，不论春秋，不分白昼，就在那每时每刻，每分每秒，每一个呼吸，等你。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【爱情语句】</td>\n",
       "      <td>那天，你问我下雨了吗，我在心里默默告诉自己，那天下了两滴雨，一滴滴在你心里，一滴滴在我心里。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                                                1\n",
       "0    type                                              sen\n",
       "1  【爱情语句】                恬恬静静温柔，端端正正品优，实实在在德厚。清清秀秀，纯纯洁洁明眸。\n",
       "2  【爱情语句】    你喜欢长长的夏季吗?因为一阵风，因为一场梦。哈哈 ，我喜欢你。清晨，午后和伴晚。都喜欢你!\n",
       "3  【爱情语句】    我喜欢你，所以静静的等你，不论春秋，不分白昼，就在那每时每刻，每分每秒，每一个呼吸，等你。\n",
       "4  【爱情语句】   那天，你问我下雨了吗，我在心里默默告诉自己，那天下了两滴雨，一滴滴在你心里，一滴滴在我心里。"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载训练集到变量中\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv('testcsens1129.txt', sep='\\t', header=None)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type 1\n",
      "【优美语句】 421\n",
      "【伤感语句】 218\n",
      "【励志的话】 351\n",
      "【爱情语句】 371\n"
     ]
    }
   ],
   "source": [
    "#查看训练集每个分类的名字以及样本数量\n",
    "for type,group in train_df.groupby(0):\n",
    "    print(type,len(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前50个句子分词共花费0.04秒\n",
      "前100个句子分词共花费0.07秒\n",
      "前150个句子分词共花费0.10秒\n",
      "前200个句子分词共花费0.13秒\n",
      "前250个句子分词共花费0.16秒\n",
      "前300个句子分词共花费0.19秒\n",
      "前350个句子分词共花费0.23秒\n",
      "前400个句子分词共花费0.27秒\n",
      "前450个句子分词共花费0.30秒\n",
      "前500个句子分词共花费0.34秒\n",
      "前550个句子分词共花费0.37秒\n",
      "前600个句子分词共花费0.42秒\n",
      "前650个句子分词共花费0.46秒\n",
      "前700个句子分词共花费0.50秒\n",
      "前750个句子分词共花费0.54秒\n",
      "前800个句子分词共花费0.57秒\n",
      "前850个句子分词共花费0.60秒\n",
      "前900个句子分词共花费0.63秒\n",
      "前950个句子分词共花费0.67秒\n",
      "前1000个句子分词共花费0.71秒\n",
      "前1050个句子分词共花费0.74秒\n",
      "前1100个句子分词共花费0.78秒\n",
      "前1150个句子分词共花费0.81秒\n",
      "前1200个句子分词共花费0.84秒\n",
      "前1250个句子分词共花费0.87秒\n",
      "前1300个句子分词共花费0.89秒\n",
      "前1350个句子分词共花费0.91秒\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['sen'],\n",
       " [' ',\n",
       "  '恬',\n",
       "  '恬静',\n",
       "  '静',\n",
       "  '温柔',\n",
       "  '端端正正',\n",
       "  '品优',\n",
       "  '实实在在',\n",
       "  '德厚',\n",
       "  '清',\n",
       "  '清秀',\n",
       "  '秀',\n",
       "  '纯',\n",
       "  '纯洁',\n",
       "  '洁',\n",
       "  '明眸'],\n",
       " [' ', '喜欢', '长长的', '夏季', '一阵风', '一场', '梦', ' ', '喜欢', '清晨', '午后', '伴晚', '喜欢'],\n",
       " [' ', '喜欢', '静静的', '春秋', '分', '白昼', '每时每刻', '每分', '每秒', '一个', '呼吸'],\n",
       " [' ', '那天', '问', '下雨', '默默', '告诉', '天下', '两滴', '雨', '滴滴', '滴滴'],\n",
       " [' ', '从来不', '记得', '回家', '路', '总会', '地方', '傻傻'],\n",
       " [' ', '追', '公交车', '追', '走', '很慢', '可爱'],\n",
       " [' ', '可知', '百年', '孤寂', '只为', '一人', '守候', '千夜', '恋歌', '只为', '一人', '唱'],\n",
       " [' ', '天空', '少', '色彩', ';', '世界', '一种', '思念'],\n",
       " [' ', '做好', '一辈子', '打算', '做好', '随时', '走', '大概', '爱情', '观', '深情', '纠缠'],\n",
       " [' ', '一生', '很多', '幸运', '遇见', '想', '留住', '幸运'],\n",
       " [' ', '白发苍苍', '容颜', '迟暮', '牵', '双手', '倾世', '温柔'],\n",
       " [' ',\n",
       "  '我行',\n",
       "  '地方',\n",
       "  '桥',\n",
       "  '看过',\n",
       "  '次数',\n",
       "  '云',\n",
       "  '喝',\n",
       "  '种类',\n",
       "  '酒',\n",
       "  '爱过',\n",
       "  '一个',\n",
       "  '正当',\n",
       "  '年龄'],\n",
       " [' ', '早上', '阳光', '想要', '未来'],\n",
       " [' ', '喜欢', '笑容', '喜欢', '静静的', '看着', '忧愁', '云', '飞去'],\n",
       " [' ', '嘴巴', '情话', '身上', '阳光', '脑子里', '爱情', '灵魂', '中有', '慌乱', '有个'],\n",
       " [' ', '不了', '世界'],\n",
       " [' ',\n",
       "  '一生',\n",
       "  '至少',\n",
       "  '忘',\n",
       "  '不求',\n",
       "  '求',\n",
       "  '同行',\n",
       "  '求',\n",
       "  '拥有',\n",
       "  '不求',\n",
       "  '爱',\n",
       "  ';',\n",
       "  '只求',\n",
       "  '最美',\n",
       "  '华里'],\n",
       " [' ',\n",
       "  '最想',\n",
       "  '牵',\n",
       "  '手',\n",
       "  '想念',\n",
       "  '笑脸',\n",
       "  '时间',\n",
       "  '改变',\n",
       "  '容颜',\n",
       "  '始终',\n",
       "  '改变',\n",
       "  '不了',\n",
       "  '我爱你',\n",
       "  '诺言',\n",
       "  '亲爱',\n",
       "  '我爱你',\n",
       "  '一生不变'],\n",
       " [' ', '陪', '经历', '苦难', '伤痛', '绝望', '显得', '不用', '生命', '阳光', '归宿'],\n",
       " [' ', '喜欢', '一个', '感觉', '大概', '听', '别人', '讨论', '爱情', '想起'],\n",
       " [' ',\n",
       "  '蜡烛',\n",
       "  '希望',\n",
       "  '火',\n",
       "  '我愿',\n",
       "  '燃烧',\n",
       "  '照亮',\n",
       "  '未来',\n",
       "  '路',\n",
       "  ';',\n",
       "  '鱼儿',\n",
       "  '希望',\n",
       "  '海',\n",
       "  '感觉',\n",
       "  '不到',\n",
       "  '泪',\n",
       "  '至少',\n",
       "  '心中'],\n",
       " [' ',\n",
       "  '玫瑰',\n",
       "  '美',\n",
       "  '比不上',\n",
       "  '笑脸',\n",
       "  ';',\n",
       "  '美酒',\n",
       "  '很香',\n",
       "  '比不上',\n",
       "  '芳香',\n",
       "  ';',\n",
       "  '大海',\n",
       "  '很深',\n",
       "  '比不上',\n",
       "  '情深',\n",
       "  ';',\n",
       "  '天空',\n",
       "  '很广',\n",
       "  '比不上',\n",
       "  '爱广',\n",
       "  '在我心中',\n",
       "  '永远',\n",
       "  '爱'],\n",
       " [' ',\n",
       "  '当风',\n",
       "  '追云',\n",
       "  '当冰',\n",
       "  '化水',\n",
       "  '当火',\n",
       "  '炽热',\n",
       "  '当石',\n",
       "  '坚硬',\n",
       "  '世上',\n",
       "  '爱情',\n",
       "  '才能',\n",
       "  '停止',\n",
       "  '爱'],\n",
       " [' ', '婚姻', '爱情', '坟墓', '见到', '这块', '墓地', '开垦', '成', '花园'],\n",
       " [' ', '抑郁', '开心果', '忧伤', '我愿', '忘忧', '树'],\n",
       " [' ',\n",
       "  '牵',\n",
       "  '手',\n",
       "  '朝朝暮暮',\n",
       "  '牵',\n",
       "  '手',\n",
       "  '等待',\n",
       "  '牵',\n",
       "  '手',\n",
       "  '走过',\n",
       "  '今生',\n",
       "  '牵',\n",
       "  '手',\n",
       "  '生生世世'],\n",
       " [' ', '感恩', '走过', '愿', '不忘', '初心', '前行'],\n",
       " [' ', '刚好', '我养', '旁边', '懒猪', '整整', '一年', '竟然', '不错'],\n",
       " [' ', '一辈子', '将来', '换', '别人']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import time\n",
    "\n",
    "train_df.columns = ['type', 'sen']\n",
    "stopword_list = [k.strip() for k in open('chineseStopWords.txt', encoding='utf8').readlines() if k.strip() != '']\n",
    "cutWords_list = []\n",
    "i = 0\n",
    "startTime = time.time()\n",
    "for article in train_df['sen']:\n",
    "    cutWords = [k for k in jieba.cut(article) if k not in stopword_list]\n",
    "    i += 1\n",
    "    if i % 50 == 0:\n",
    "        print('前%d个句子分词共花费%.2f秒' %(i, time.time()-startTime))\n",
    "    cutWords_list.append(cutWords)\n",
    "cutWords_list[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cutWords_list.txt', 'w') as file:\n",
    "    for cutWords in cutWords_list:\n",
    "        file.write(' '.join(cutWords) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运用word2vec训练词向量模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化词向量模型对象\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(cutWords_list, size=100, iter=10, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 0.9995880126953125),\n",
       " ('一个', 0.9995020627975464),\n",
       " ('好', 0.9994383454322815),\n",
       " ('想', 0.9993307590484619),\n",
       " ('希望', 0.999306857585907),\n",
       " ('爱', 0.9992737770080566),\n",
       " ('生活', 0.9992696046829224),\n",
       " ('事', 0.9992365837097168),\n",
       " ('幸福', 0.9991884827613831),\n",
       " ('笑', 0.9991754293441772)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar('目标')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('生活', 0.9979250431060791)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar(positive=['爱情', '心'], negative=['目标'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '抑郁' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5d47603f97bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"爱情\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"抑郁\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/bigdata2/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m         \"\"\"\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/bigdata2/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/bigdata2/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/bigdata2/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '抑郁' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "word2vec_model.wv.similarity(\"爱情\",\"抑郁\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata2",
   "language": "python",
   "name": "bigdata2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
